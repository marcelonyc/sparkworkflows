{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function, NewTask, get_run_db, mlconf, mount_v3io, new_model_server, builder\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "from os import path\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "# specify artifacts target location\n",
    "artifact_path = mlconf.artifact_path or path.abspath('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment vars to be set by Nuclio\n",
    "#PYTHON_SCRIPT = os.getenv('PYTHON_SCRIPT','/kv-to-parquet.py')\n",
    "PYTHON_SCRIPT = os.getenv('PYTHON_SCRIPT','/pi.py')\n",
    "V3IO_SCRIPT_PATH = os.getenv('V3IO_SCRIPT_PATH',os.getcwd().replace('/User','/v3io/'+os.getenv('V3IO_HOME')))\n",
    "\n",
    "SPARK_JOB_NAME = os.getenv('SPARK_JOB_NAME','my-spark-job') \n",
    "SPARK_SPEC_MEM = os.getenv('SPARK_SPEC_MEM','2g') \n",
    "SPARK_SPEC_CPU = int(os.getenv('SPARK_SPEC_CPU',1) )\n",
    "SPARK_SPEC_REPLICAS = int(os.getenv('SPARK_SPEC_REPLICAS',1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the pyspark script path\n",
    "V3IO_SCRIPT_PATH = V3IO_SCRIPT_PATH+PYTHON_SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the list of the dpendency jars\n",
    "V3IO_JARS_PATH = '/igz/java/libs/'\n",
    "DEPS_JARS_LIST = [join(V3IO_JARS_PATH, f) for f in os.listdir(V3IO_JARS_PATH) \n",
    "                  if isfile(join(V3IO_JARS_PATH, f)) and f.startswith('v3io-') and f.endswith('.jar')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MLRun function to run the spark-job on the kubernetes cluster\n",
    "serverless_spark_fn = new_function(kind='spark', image='urihoenig/spark-app:2.4.4-2.9.0-0.0.3', \n",
    "                                   command=f'local://{V3IO_SCRIPT_PATH}', name=SPARK_JOB_NAME).apply(mount_v3io(name='v3io', remote='~/', mount_path='/User', access_key=os.getenv('V3IO_ACCESS_KEY'),\n",
    "      user=os.getenv('V3IO_USERNAME')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverless_spark_fn.spec.env.append({'name':'V3IO_HOME_URL','value':os.getenv(\"V3IO_HOME_URL\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverless_spark_fn.with_limits(mem=SPARK_SPEC_MEM)\n",
    "serverless_spark_fn.with_requests(cpu=SPARK_SPEC_CPU)\n",
    "serverless_spark_fn.with_igz_spark(igz_version='2.8_b3506_20191217042239')\n",
    "#Set number of executors\n",
    "serverless_spark_fn.spec.replicas = SPARK_SPEC_REPLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run spark script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-06-01 14:41:21,274 artifact path is not defined or is local, artifacts will not be visible in the UI\n",
      "[mlrun] 2020-06-01 14:41:21,285 starting run my-spark-job uid=11cd0a9eb250441ebf600e3eddce8ed5  -> http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_K8S_CMD=driver-py\n",
      "+ case \"$SPARK_K8S_CMD\" in\n",
      "+ shift 1\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -n file:///igz/java/libs/v3io-py.zip ']'\n",
      "+ PYTHONPATH=:file:///igz/java/libs/v3io-py.zip\n",
      "+ PYSPARK_ARGS=\n",
      "+ '[' -n '' ']'\n",
      "+ R_ARGS=\n",
      "+ '[' -n '' ']'\n",
      "+ '[' 2 == 2 ']'\n",
      "++ python -V\n",
      "+ pyv='Python 3.6.8'\n",
      "+ export PYTHON_VERSION=3.6.8\n",
      "+ PYTHON_VERSION=3.6.8\n",
      "+ export PYSPARK_PYTHON=python\n",
      "+ PYSPARK_PYTHON=python\n",
      "+ export PYSPARK_DRIVER_PYTHON=python\n",
      "+ PYSPARK_DRIVER_PYTHON=python\n",
      "+ case \"$SPARK_K8S_CMD\" in\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\" $PYSPARK_PRIMARY $PYSPARK_ARGS)\n",
      "+ exec /sbin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.200.0.61 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner /v3io/users/admin/sparkworkflows/pi.py\n",
      "\n",
      "20/06/01 14:42:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/06/01 14:42:09 INFO spark.SparkContext: Running Spark version 2.4.4\n",
      "20/06/01 14:42:09 INFO spark.SparkContext: Submitted application: PythonPi\n",
      "20/06/01 14:42:09 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "20/06/01 14:42:09 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "20/06/01 14:42:09 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/01 14:42:09 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/01 14:42:09 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "20/06/01 14:42:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "20/06/01 14:42:10 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "20/06/01 14:42:10 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "20/06/01 14:42:10 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/06/01 14:42:10 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/06/01 14:42:10 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/blockmgr-aa16a497-4cbf-4fce-844f-22b9ef298e6c\n",
      "20/06/01 14:42:10 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MB\n",
      "20/06/01 14:42:10 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "20/06/01 14:42:10 INFO util.log: Logging initialized @6775ms\n",
      "20/06/01 14:42:10 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/06/01 14:42:10 INFO server.Server: Started @6986ms\n",
      "20/06/01 14:42:10 INFO server.AbstractConnector: Started ServerConnector@30c5e824{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/06/01 14:42:10 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a986242{/jobs,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bd29c70{/jobs/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4978116b{/jobs/job,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31575113{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ee97a55{/stages,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3793d7aa{/stages/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b9a371e{/stages/stage,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c8770a4{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60f80ed{/stages/pool,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@735ad0d{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ffe553{/storage,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@136aa738{/storage/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69184550{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@436fd0bb{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38c170f8{/environment,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ed443bf{/environment/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@554dbbf0{/executors,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@689a43ea{/executors/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5befa8b1{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fac1c8{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fd068fc{/static,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1daf5a68{/,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@253b439d{/api,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e73f004{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@261288e{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:11 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:4040\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added JAR file:///igz/java/libs/v3io-hcfs_2.11-2.8_b3506_20191217042239.jar at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/jars/v3io-hcfs_2.11-2.8_b3506_20191217042239.jar with timestamp 1591022531140\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added JAR file:///igz/java/libs/v3io-spark2-streaming_2.11-2.8_b3506_20191217042239.jar at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/jars/v3io-spark2-streaming_2.11-2.8_b3506_20191217042239.jar with timestamp 1591022531141\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added JAR file:///igz/java/libs/v3io-spark2-object-dataframe_2.11-2.8_b3506_20191217042239.jar at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/jars/v3io-spark2-object-dataframe_2.11-2.8_b3506_20191217042239.jar with timestamp 1591022531141\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added JAR file:///igz/java/libs/scala-library-2.11.12.jar at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/jars/scala-library-2.11.12.jar with timestamp 1591022531141\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added file file:///igz/java/libs/v3io-py-2.8_b3506_20191217042239.zip at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/files/v3io-py-2.8_b3506_20191217042239.zip with timestamp 1591022531323\n",
      "20/06/01 14:42:11 INFO util.Utils: Copying /igz/java/libs/v3io-py-2.8_b3506_20191217042239.zip to /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/spark-5bbed2d6-d86d-447d-bb66-a38f4df689b2/userFiles-d3faeb0f-9017-48a9-b821-4ed5d540d2f0/v3io-py-2.8_b3506_20191217042239.zip\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added file file:///v3io/users/admin/sparkworkflows/pi.py at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/files/pi.py with timestamp 1591022531342\n",
      "20/06/01 14:42:11 INFO util.Utils: Copying /v3io/users/admin/sparkworkflows/pi.py to /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/spark-5bbed2d6-d86d-447d-bb66-a38f4df689b2/userFiles-d3faeb0f-9017-48a9-b821-4ed5d540d2f0/pi.py\n",
      "20/06/01 14:42:11 INFO spark.SparkContext: Added file file:///igz/java/libs/v3io-py.zip at spark://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7078/files/v3io-py.zip with timestamp 1591022531434\n",
      "20/06/01 14:42:11 INFO util.Utils: Copying /igz/java/libs/v3io-py.zip to /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/spark-5bbed2d6-d86d-447d-bb66-a38f4df689b2/userFiles-d3faeb0f-9017-48a9-b821-4ed5d540d2f0/v3io-py.zip\n",
      "20/06/01 14:42:11 WARN spark.SparkContext: The path file:///igz/java/libs/v3io-py.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "20/06/01 14:42:14 INFO k8s.ExecutorPodsAllocator: Going to request 1 executors from Kubernetes.\n",
      "20/06/01 14:42:14 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "20/06/01 14:42:14 INFO netty.NettyBlockTransferService: Server created on my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7079\n",
      "20/06/01 14:42:14 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/06/01 14:42:14 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/06/01 14:42:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7079 with 93.3 MB RAM, BlockManagerId(driver, my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/06/01 14:42:14 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/06/01 14:42:14 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/06/01 14:42:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46eea553{/metrics/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:22 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.63:43942) with ID 1\n",
      "20/06/01 14:42:22 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "20/06/01 14:42:22 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.63:41077 with 997.8 MB RAM, BlockManagerId(1, 10.200.0.63, 41077, None)\n",
      "20/06/01 14:42:22 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark/spark-warehouse').\n",
      "20/06/01 14:42:22 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "20/06/01 14:42:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b63345b{/SQL,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e5d2342{/SQL/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dd3f56c{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d69cc1{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@413ddd50{/static/sql,null,AVAILABLE,@Spark}\n",
      "20/06/01 14:42:23 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/06/01 14:42:24 INFO spark.SparkContext: Starting job: reduce at /v3io/users/admin/sparkworkflows/pi.py:47\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Got job 0 (reduce at /v3io/users/admin/sparkworkflows/pi.py:47) with 2 output partitions\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at /v3io/users/admin/sparkworkflows/pi.py:47)\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /v3io/users/admin/sparkworkflows/pi.py:47), which has no missing parents\n",
      "20/06/01 14:42:24 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.1 KB, free 93.3 MB)\n",
      "20/06/01 14:42:24 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 93.3 MB)\n",
      "20/06/01 14:42:24 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:7079 (size: 4.2 KB, free: 93.3 MB)\n",
      "20/06/01 14:42:24 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n",
      "20/06/01 14:42:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /v3io/users/admin/sparkworkflows/pi.py:47) (first 15 tasks are for partitions Vector(0, 1))\n",
      "20/06/01 14:42:24 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "20/06/01 14:42:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.200.0.63, executor 1, partition 0, PROCESS_LOCAL, 7871 bytes)\n",
      "20/06/01 14:42:24 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.63:41077 (size: 4.2 KB, free: 997.8 MB)\n",
      "20/06/01 14:42:25 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.200.0.63, executor 1, partition 1, PROCESS_LOCAL, 7871 bytes)\n",
      "20/06/01 14:42:25 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1186 ms on 10.200.0.63 (executor 1) (1/2)\n",
      "20/06/01 14:42:25 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60989\n",
      "20/06/01 14:42:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 156 ms on 10.200.0.63 (executor 1) (2/2)\n",
      "20/06/01 14:42:25 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/06/01 14:42:25 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at /v3io/users/admin/sparkworkflows/pi.py:47) finished in 1.607 s\n",
      "20/06/01 14:42:25 INFO scheduler.DAGScheduler: Job 0 finished: reduce at /v3io/users/admin/sparkworkflows/pi.py:47, took 1.646125 s\n",
      "[mlrun] 2020-06-01 14:42:25,708 ++================++\n",
      "[mlrun] 2020-06-01 14:42:25,708 Pi is roughly 3.130880\n",
      "[mlrun] 2020-06-01 14:42:25,708 ++================++\n",
      "20/06/01 14:42:25 INFO server.AbstractConnector: Stopped Spark@30c5e824{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/06/01 14:42:25 INFO ui.SparkUI: Stopped Spark web UI at http://my-spark-job-124cabea-1591022494836-driver-svc.default-tenant.svc:4040\n",
      "20/06/01 14:42:25 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "20/06/01 14:42:25 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "20/06/01 14:42:25 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "20/06/01 14:42:25 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/06/01 14:42:25 INFO memory.MemoryStore: MemoryStore cleared\n",
      "20/06/01 14:42:25 INFO storage.BlockManager: BlockManager stopped\n",
      "20/06/01 14:42:25 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/06/01 14:42:25 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/06/01 14:42:25 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "20/06/01 14:42:26 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/06/01 14:42:26 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-772d72a1-f51f-4156-9c07-609ee94ef0e9\n",
      "20/06/01 14:42:26 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/spark-5bbed2d6-d86d-447d-bb66-a38f4df689b2/pyspark-79c54256-0652-417a-a266-3043bc1d6af1\n",
      "20/06/01 14:42:26 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-d6b8c114-e6b1-45ac-899d-360f98cee314/spark-5bbed2d6-d86d-447d-bb66-a38f4df689b2\n",
      "final state: succeeded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"11cd0a9eb250441ebf600e3eddce8ed5\"><a href=\"https://mlrun-ui.default-tenant.app.mdl0521.iguazio-cd0.com/projects/default/jobs/11cd0a9eb250441ebf600e3eddce8ed5/info\" target=\"_blank\" >...ddce8ed5</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jun 01 14:41:21</td>\n",
       "      <td>completed</td>\n",
       "      <td>my-spark-job</td>\n",
       "      <td><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">v3io_user=admin</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resulta1ed4f1c-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resulta1ed4f1c-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resulta1ed4f1c\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resulta1ed4f1c-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run 11cd0a9eb250441ebf600e3eddce8ed5 --project default , !mlrun logs 11cd0a9eb250441ebf600e3eddce8ed5 --project default\n",
      "[mlrun] 2020-06-01 14:42:28,953 run executed, status=completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7f1606461da0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serverless_spark_fn.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Spark sript as part of a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = \"label\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Kubeflow pipeline with Spark jobs',\n",
    "    description='Run SparkK8s as par tof pipeline'\n",
    ")\n",
    "def example_pipeline(\n",
    "   p1 = [1,2,3,4,5,6],\n",
    "   p2 = [9,8,6,5,4,3]\n",
    "):\n",
    "    # Use the same fn definition, but run different functions.\n",
    "    # fn2 is a definition from the external notebook\n",
    "    f1 = serverless_spark_fn.as_step(NewTask(), name='Sparkstep1',outputs=['bankxact']).apply(mount_v3io(name='v3io', remote='~/', mount_path='/User', access_key=os.getenv('V3IO_ACCESS_KEY'),\n",
    "      user=os.getenv('V3IO_USERNAME')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace='default-tenant')\n",
    "p1 = [1,2,3,4,5,6]\n",
    "p2 = [9,8,6,5,4,3]\n",
    "#arguments = {'p1': p1 ,'p2': p2}\n",
    "arguments={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record pipeline deployment in KV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import v3io.dataplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3io_client = v3io.dataplane.Client(max_connections=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_pipeline_id(run_id):\n",
    "    record_id=str(run_id)\n",
    "    v3io_client.put_item(container=os.getenv('MONITOR_CONTAINER','bigdata'),\n",
    "                         path=os.path.join(os.getenv('MONITOR_TABLE','kubeflow_runs'),record_id),\n",
    "                         attributes={\n",
    "                             'id': record_id,\n",
    "                             'status' : 'started',}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the Kubeflow run_id\n",
    "Note: This notebook was written to invoke the deployment as an mlrun function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context,event):\n",
    "    run_id=str(uuid.uuid4())\n",
    "    run_result = client.create_run_from_pipeline_func(example_pipeline, arguments, run_name='SparkPipe-'+run_id, experiment_name='SparkPipeline')\n",
    "    record_pipeline_id(run_result.run_id)\n",
    "    return run_result.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"[1, 2, 3, 4, 5, 6]\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/conda/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"[9, 8, 6, 5, 4, 3]\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.mdl0521.iguazio-cd0.com/pipelines/#/experiments/details/9d9d1ec1-ed02-4afb-93a9-57d4ba2799c5\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.mdl0521.iguazio-cd0.com/pipelines/#/runs/details/7fe97143-15fe-4486-8d12-d94177807c20\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7fe97143-15fe-4486-8d12-d94177807c20'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler('1','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
